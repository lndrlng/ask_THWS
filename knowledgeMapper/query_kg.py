# query_kg.py

import asyncio
from lightrag import LightRAG, QueryParam
from local_models import HFEmbedFunc, OllamaLLM


async def main():
    print("ğŸ“‚ Loading LightRAG storage from ./rag_storage")
    rag = LightRAG(
        working_dir="./rag_storage",
        embedding_func=HFEmbedFunc(),
        llm_model_func=OllamaLLM()
    )
    await rag.initialize_storages()

    while True:
        question = input("â“ Ask a question (or type 'exit'): ").strip()
        if question.lower() in {"exit", "quit"}:
            break

        print("ğŸ¤– Thinking...")
        result = await rag.aquery(question, param=QueryParam(mode="hybrid"))
        print("ğŸ’¬ Answer:", result)


asyncio.run(main())
